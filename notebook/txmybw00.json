{
	"name": "txmybw00",
	"properties": {
		"folder": {
			"name": "E2E Lakehouse HC/1 - Bronze"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool1",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "68d1d94f-d2ca-4c67-91ab-63403b5ba653"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/8a936955-932a-425a-8c07-f6035412c4a1/resourceGroups/rg_synapse/providers/Microsoft.Synapse/workspaces/nmdemosynapse/bigDataPools/sparkpool1",
				"name": "sparkpool1",
				"type": "Spark",
				"endpoint": "https://nmdemosynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool1",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Load incrementally into Bronze Delta Table \r\n",
					"Hard-coded version for demonstration purposes\r\n",
					"\r\n",
					"Here is a good guide on how to use spark notebooks https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-development-using-notebooks\r\n",
					"______________________________________________"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Load parquet extract into spark dataframe"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					},
					"collapsed": false
				},
				"source": [
					"%%pyspark\r\n",
					"df = spark.read.load('abfss://rawdata@nmdemostorage1.dfs.core.windows.net/landing/salesrusdb1/txmybw00.parquet', format='parquet')\r\n",
					"display(df.limit(3))"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Register spark dataframe using _createOrReplaceTempView_ method to make it available for Spark SQL syntax"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df.createOrReplaceTempView(\"v_tmp\")"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Create the initial delta files if they don't exist (on the first load only)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"try:\r\n",
					"    mssparkutils.fs.ls('abfss://bronze@nmdemostorage1.dfs.core.windows.net/delta_files/transaction_history')\r\n",
					"    print('Delta Files exist')\r\n",
					"except:\r\n",
					"    print('Creating initial table')\r\n",
					"    # write out table as delta\r\n",
					"    df.write.format('delta').mode(\"overwrite\").save('abfss://bronze@nmdemostorage1.dfs.core.windows.net/delta_files/transaction_history')"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Now we need to expose these delta files in our lake database to easily query with Serverless and Spark SQL. We basically create metadata about the location of the delta files, and wrap it in familiar db.table reference.\r\n",
					"\r\n",
					"More info here https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"-- create the lake database if it does not exist\r\n",
					"CREATE DATABASE IF NOT EXISTS bronze"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"-- create the delta reference\r\n",
					"CREATE TABLE IF NOT EXISTS bronze.transaction_history\r\n",
					"    USING DELTA\r\n",
					"--     [ OPTIONS ( key1=val1, key2=val2, ... ) ]\r\n",
					"--     [ PARTITIONED BY ( col_name1, col_name2, ... ) ]\r\n",
					"--     [ CLUSTERED BY ( col_name3, col_name4, ... ) \r\n",
					"--         [ SORTED BY ( col_name [ ASC | DESC ], ... ) ] \r\n",
					"--         INTO num_buckets BUCKETS ]\r\n",
					"    LOCATION 'abfss://bronze@nmdemostorage1.dfs.core.windows.net/delta_files/transaction_history'\r\n",
					"--     [ COMMENT table_comment ]"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"If the initial table already exists, we just need to merge into the existing table.\r\n",
					"\r\n",
					"Merge into syntax docs here https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/delta-merge-into#syntax"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					" %%sql \r\n",
					"MERGE INTO bronze.transaction_history tgt\r\n",
					"USING v_incremental_transaction_history src\r\n",
					"ON tgt.TransactionID = src.TransactionID\r\n",
					"WHEN MATCHED THEN\r\n",
					"  UPDATE SET *\r\n",
					"WHEN NOT MATCHED\r\n",
					"  THEN INSERT *"
				],
				"execution_count": 10
			}
		]
	}
}