{
	"name": "HC Incremental Copy from On-prem SQL Spark_copy1",
	"properties": {
		"description": "using Spark",
		"activities": [
			{
				"name": "Extract as Parquet to Data Lake",
				"type": "Copy",
				"dependsOn": [],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "AzureSqlSource",
						"sqlReaderQuery": "SELECT * FROM SalesLT.SalesOrderHeader",
						"queryTimeout": "02:00:00",
						"partitionOption": "None"
					},
					"sink": {
						"type": "ParquetSink",
						"storeSettings": {
							"type": "AzureBlobFSWriteSettings"
						},
						"formatSettings": {
							"type": "ParquetWriteSettings",
							"maxRowsPerFile": 3,
							"fileNamePrefix": "sample_extract"
						}
					},
					"enableStaging": false,
					"translator": {
						"type": "TabularTranslator",
						"typeConversion": true,
						"typeConversionSettings": {
							"allowDataTruncation": true,
							"treatBooleanAsNumber": false
						}
					}
				},
				"inputs": [
					{
						"referenceName": "SqlTable",
						"type": "DatasetReference"
					}
				],
				"outputs": [
					{
						"referenceName": "ParquetOutMultiple",
						"type": "DatasetReference",
						"parameters": {
							"FileSystem": "landing",
							"Directory": "dataverse"
						}
					}
				]
			}
		],
		"folder": {
			"name": "Spark Incremental Files to Staging Ded SQL Pool"
		},
		"annotations": []
	}
}